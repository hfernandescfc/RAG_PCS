# -*- coding: utf-8 -*-
import streamlit as st
import os
import json
import requests
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="Sistema RAG - Consulta de Documentos",
    page_icon="ğŸ“š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
    .stAlert {
        margin-top: 1rem;
    }
    .fonte-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# TÃ­tulo
st.markdown('<h1 class="main-header">ğŸ“š Sistema de Consulta de Documentos - Programa Cidade Saneada</h1>', unsafe_allow_html=True)

# Sidebar - ConfiguraÃ§Ãµes
with st.sidebar:
    st.header("âš™ï¸ ConfiguraÃ§Ãµes")
    
    # Caminho do banco
    db_path = st.text_input(
        "Caminho do Banco Vetorial",
        value="./chroma_db_multiplos",
        help="Pasta onde estÃ¡ o banco vetorial"
    )
    
    # Modelo de embeddings
    embedding_model = st.selectbox(
        "Modelo de Embeddings",
        [
            "sentence-transformers/all-MiniLM-L6-v2",
            "neuralmind/bert-base-portuguese-cased",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ],
        help="Use o mesmo modelo usado no processamento"
    )
    
    # ConfiguraÃ§Ãµes de busca
    st.subheader("ğŸ” ParÃ¢metros de Busca")
    
    num_docs = st.slider(
        "NÃºmero de documentos",
        min_value=3,
        max_value=10,
        value=6,
        help="Quantos trechos buscar nos documentos"
    )
    
    search_type = st.radio(
        "Tipo de busca",
        ["MMR (Diversidade)", "Similaridade"],
        help="MMR evita resultados repetitivos"
    )
    
    # ConfiguraÃ§Ãµes do LLM
    st.subheader("ğŸ¤– Modelo de IA")
    
    ollama_model = st.selectbox(
        "Modelo Ollama",
        ["llama3.2:3b", "llama3.1:8b", "mistral", "phi3:mini"],
        help="Modelo para gerar respostas"
    )
    
    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=0.1,
        help="0 = mais preciso, 1 = mais criativo"
    )
    
    # InformaÃ§Ãµes do sistema
    st.divider()
    st.subheader("â„¹ï¸ InformaÃ§Ãµes")
    
    if os.path.exists("./arquivos_processados.json"):
        try:
            with open("./arquivos_processados.json", 'r', encoding='utf-8') as f:
                registro = json.load(f)
            
            st.metric("Documentos processados", len(registro))
            
            with st.expander("Ver documentos"):
                for hash_key, info in registro.items():
                    st.text(f"ğŸ“„ {info.get('nome', 'N/A')}")
                    st.caption(f"   PÃ¡ginas: {info.get('paginas', '?')} | MÃ©todo: {info.get('metodo', '?')}")
        except UnicodeDecodeError:
            st.warning("âš ï¸ Erro ao ler arquivos_processados.json. Verifique a codificaÃ§Ã£o do arquivo.")

# FunÃ§Ã£o de inicializaÃ§Ã£o (cache para nÃ£o recarregar sempre)
@st.cache_resource
def inicializar_sistema(db_path, embedding_model, ollama_model, temperature):
    """Inicializa o sistema RAG"""
    
    # Verificar se banco existe
    if not os.path.exists(db_path):
        st.error(f"âŒ Banco vetorial nÃ£o encontrado em: {db_path}")
        st.stop()
    
    # Verificar Ollama
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code != 200:
            raise Exception()
    except:
        st.error("âŒ Ollama nÃ£o estÃ¡ rodando! Execute: `ollama serve`")
        st.stop()
    
    # Carregar embeddings
    with st.spinner("ğŸ§  Carregando modelo de embeddings..."):
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    # Carregar banco
    with st.spinner("ğŸ’¾ Carregando banco vetorial..."):
        db = Chroma(persist_directory=db_path, embedding_function=embeddings)
    
    # Configurar LLM
    llm = Ollama(
        model=ollama_model,
        temperature=temperature,
        num_ctx=4096,
        num_predict=1024
    )
    
    # Prompt customizado
    template_prompt = """VocÃª Ã© um assistente especializado em anÃ¡lise de documentos contratuais e tÃ©cnicos.

Use APENAS as informaÃ§Ãµes dos documentos fornecidos abaixo para responder Ã  pergunta.

REGRAS IMPORTANTES:
1. Se a informaÃ§Ã£o NÃƒO estiver nos documentos, diga claramente: "NÃ£o encontrei essa informaÃ§Ã£o nos documentos fornecidos"
2. Cite especificamente de qual documento e pÃ¡gina vocÃª tirou a informaÃ§Ã£o
3. Se houver informaÃ§Ãµes conflitantes, mencione ambas e suas fontes
4. Seja preciso e objetivo
5. Use linguagem clara e profissional

DOCUMENTOS:
{context}

PERGUNTA: {question}

RESPOSTA DETALHADA:"""

    PROMPT = PromptTemplate(
        template=template_prompt,
        input_variables=["context", "question"]
    )
    
    return db, llm, PROMPT, embeddings

# Inicializar sistema
try:
    db, llm, PROMPT, embeddings = inicializar_sistema(
        db_path, 
        embedding_model, 
        ollama_model, 
        temperature
    )
    
    st.success("âœ… Sistema carregado com sucesso!")
    
except Exception as e:
    st.error(f"âŒ Erro ao inicializar: {e}")
    st.stop()

# Tabs principais
tab1, tab2, tab3 = st.tabs(["ğŸ’¬ Consulta", "ğŸ” Busca Literal", "ğŸ“Š EstatÃ­sticas"])

# ============================================================================
# TAB 1: CONSULTA PRINCIPAL
# ============================================================================
with tab1:
    st.header("FaÃ§a sua pergunta sobre os documentos")
    
    # SugestÃµes de perguntas
    with st.expander("ğŸ’¡ Exemplos de perguntas"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Perguntas diretas:**
            - Qual o prazo do contrato?
            - Quem sÃ£o as partes contratantes?
            - Qual o valor total do contrato?
            - Quais sÃ£o as obrigaÃ§Ãµes da concessionÃ¡ria?
            """)
        
        with col2:
            st.markdown("""
            **Perguntas especÃ­ficas:**
            - HÃ¡ previsÃ£o de multas?
            - Qual o prazo para universalizaÃ§Ã£o?
            - A quem cabe a elaboraÃ§Ã£o dos projetos?
            - Quais sÃ£o as garantias contratuais?
            """)
    
    # Campo de pergunta
    query = st.text_area(
        "Sua pergunta:",
        height=100,
        placeholder="Digite aqui sua pergunta sobre os documentos..."
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        buscar_btn = st.button("ğŸ” Buscar Resposta", type="primary", use_container_width=True)
    
    with col2:
        mostrar_preview = st.checkbox("Mostrar preview", value=False)
    
    with col3:
        mostrar_fontes = st.checkbox("Mostrar fontes", value=True)
    
    if buscar_btn and query:
        # Configurar retriever
        search_type_str = "mmr" if "MMR" in search_type else "similarity"
        
        if search_type_str == "mmr":
            retriever = db.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": num_docs,
                    "fetch_k": num_docs * 3,
                    "lambda_mult": 0.5
                }
            )
        else:
            retriever = db.as_retriever(
                search_type="similarity",
                search_kwargs={"k": num_docs}
            )
        
        # Preview dos documentos encontrados
        if mostrar_preview:
            with st.expander("ğŸ“„ Preview dos documentos encontrados"):
                docs_preview = retriever.invoke(query)
                
                for i, doc in enumerate(docs_preview, 1):
                    st.markdown(f"**[{i}] {doc.metadata.get('source', '?')} (pÃ¡g. {doc.metadata.get('page', '?')})**")
                    st.caption(doc.page_content[:200] + "...")
                    st.divider()
        
        # Criar chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # Buscar resposta
        with st.spinner("ğŸ¤– Gerando resposta... (isso pode levar 10-30 segundos)"):
            try:
                result = qa_chain.invoke({"query": query})
                
                # Mostrar resposta
                st.markdown("### ğŸ“ Resposta:")
                st.info(result['result'])
                
                # Mostrar fontes
                if mostrar_fontes and result.get('source_documents'):
                    st.markdown("### ğŸ“š Fontes utilizadas:")
                    
                    for i, doc in enumerate(result['source_documents'], 1):
                        with st.container():
                            col1, col2, col3 = st.columns([3, 1, 1])
                            
                            with col1:
                                st.markdown(f"**[{i}] {doc.metadata.get('source', 'Desconhecido')}**")
                            
                            with col2:
                                st.caption(f"PÃ¡gina {doc.metadata.get('page', '?')}")
                            
                            with col3:
                                metodo = doc.metadata.get('metodo', '?')
                                badge_color = "ğŸŸ¢" if metodo == "extraÃ§Ã£o_direta" else "ğŸ”µ"
                                st.caption(f"{badge_color} {metodo}")
                            
                            with st.expander("Ver trecho"):
                                st.text(doc.page_content[:500] + "...")
                
                # Salvar no histÃ³rico
                if 'historico' not in st.session_state:
                    st.session_state.historico = []
                
                st.session_state.historico.append({
                    'timestamp': datetime.now().isoformat(),
                    'pergunta': query,
                    'resposta': result['result'],
                    'num_fontes': len(result.get('source_documents', []))
                })
                
            except Exception as e:
                st.error(f"âŒ Erro ao gerar resposta: {e}")

# ============================================================================
# TAB 2: BUSCA LITERAL
# ============================================================================
with tab2:
    st.header("ğŸ” Busca literal nos documentos")
    st.caption("Busca por termos exatos no texto dos documentos")
    
    termo_busca = st.text_input("Digite o termo para buscar:", placeholder="Ex: universalizaÃ§Ã£o")
    
    if st.button("ğŸ” Buscar Termo", use_container_width=True):
        if termo_busca:
            with st.spinner(f"Buscando '{termo_busca}'..."):
                all_docs = db.get()
                encontrados = []
                
                for doc_id, text, metadata in zip(
                    all_docs['ids'], 
                    all_docs['documents'], 
                    all_docs['metadatas']
                ):
                    if termo_busca.lower() in text.lower():
                        encontrados.append((text, metadata))
                
                if encontrados:
                    st.success(f"âœ… Encontrado em {len(encontrados)} trechos")
                    
                    for i, (text, meta) in enumerate(encontrados[:20], 1):  # Mostrar atÃ© 20
                        source = meta.get('source', '?')
                        page = meta.get('page', '?')
                        
                        with st.expander(f"[{i}] {source} - PÃ¡gina {page}"):
                            # Destacar termo
                            idx = text.lower().find(termo_busca.lower())
                            preview_start = max(0, idx - 100)
                            preview_end = min(len(text), idx + 200)
                            preview = text[preview_start:preview_end]
                            
                            st.text(f"...{preview}...")
                    
                    if len(encontrados) > 20:
                        st.info(f"Mostrando 20 de {len(encontrados)} resultados")
                else:
                    st.warning(f"âŒ Termo '{termo_busca}' nÃ£o encontrado")

# ============================================================================
# TAB 3: ESTATÃSTICAS
# ============================================================================
with tab3:
    st.header("ğŸ“Š EstatÃ­sticas do Sistema")
    
    # InformaÃ§Ãµes do banco
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if os.path.exists("./arquivos_processados.json"):
            try:
                with open("./arquivos_processados.json", 'r', encoding='utf-8') as f:
                    registro = json.load(f)
                st.metric("ğŸ“„ Documentos", len(registro))
            except:
                st.metric("ğŸ“„ Documentos", "Erro")
    
    with col2:
        all_docs = db.get()
        st.metric("ğŸ“¦ Chunks", len(all_docs['ids']))
    
    with col3:
        if 'historico' in st.session_state:
            st.metric("ğŸ’¬ Consultas", len(st.session_state.historico))
    
    # HistÃ³rico de consultas
    if 'historico' in st.session_state and st.session_state.historico:
        st.subheader("ğŸ“ HistÃ³rico de Consultas")
        
        for i, item in enumerate(reversed(st.session_state.historico[-10:]), 1):
            with st.expander(f"{i}. {item['pergunta'][:80]}..."):
                st.caption(f"ğŸ• {item['timestamp']}")
                st.markdown(f"**Pergunta:** {item['pergunta']}")
                st.markdown(f"**Resposta:** {item['resposta']}")
                st.caption(f"ğŸ“š Fontes consultadas: {item['num_fontes']}")
        
        if st.button("ğŸ—‘ï¸ Limpar HistÃ³rico"):
            st.session_state.historico = []
            st.rerun()
    else:
        st.info("Nenhuma consulta realizada ainda")
    
    # InformaÃ§Ãµes tÃ©cnicas
    with st.expander("ğŸ”§ InformaÃ§Ãµes TÃ©cnicas"):
        st.json({
            "Banco Vetorial": db_path,
            "Modelo Embeddings": embedding_model,
            "Modelo LLM": ollama_model,
            "Temperatura": temperature,
            "Tipo de Busca": search_type,
            "Documentos por Busca": num_docs
        })