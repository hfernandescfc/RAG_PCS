import streamlit as st
import os
import json
import requests
from datetime import datetime
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# Configura√ß√£o da p√°gina
st.set_page_config(
    page_title="Sistema RAG - Consulta de Documentos",
    page_icon="üìö",
    layout="wide",
    initial_sidebar_state="expanded"
)

# CSS customizado
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        color: #1f77b4;
        text-align: center;
        padding: 1rem 0;
    }
    .stAlert {
        margin-top: 1rem;
    }
    .fonte-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
</style>
""", unsafe_allow_html=True)

# T√≠tulo
st.markdown('<h1 class="main-header">üìö Sistema de Consulta de Documentos</h1>', unsafe_allow_html=True)

# Sidebar - Configura√ß√µes
with st.sidebar:
    st.header("‚öôÔ∏è Configura√ß√µes")
    
    # Caminho do banco
    db_path = st.text_input(
        "Caminho do Banco Vetorial",
        value="./chroma_db_multiplos",
        help="Pasta onde est√° o banco vetorial"
    )
    
    # Modelo de embeddings
    embedding_model = st.selectbox(
        "Modelo de Embeddings",
        [
            "sentence-transformers/all-MiniLM-L6-v2",
            "neuralmind/bert-base-portuguese-cased",
            "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
        ],
        help="Use o mesmo modelo usado no processamento"
    )
    
    # Configura√ß√µes de busca
    st.subheader("üîç Par√¢metros de Busca")
    
    num_docs = st.slider(
        "N√∫mero de documentos",
        min_value=3,
        max_value=10,
        value=6,
        help="Quantos trechos buscar nos documentos"
    )
    
    search_type = st.radio(
        "Tipo de busca",
        ["MMR (Diversidade)", "Similaridade"],
        help="MMR evita resultados repetitivos"
    )
    
    # Configura√ß√µes do LLM
    st.subheader("ü§ñ Modelo de IA")
    
    ollama_model = st.selectbox(
        "Modelo Ollama",
        ["llama3.2:3b", "llama3.1:8b", "mistral", "phi3:mini"],
        help="Modelo para gerar respostas"
    )
    
    temperature = st.slider(
        "Temperatura",
        min_value=0.0,
        max_value=1.0,
        value=0.0,
        step=0.1,
        help="0 = mais preciso, 1 = mais criativo"
    )
    
    # Informa√ß√µes do sistema
    st.divider()
    st.subheader("‚ÑπÔ∏è Informa√ß√µes")
    
    if os.path.exists("./arquivos_processados.json"):
        with open("./arquivos_processados.json", 'r', encoding='utf-8') as f:
            registro = json.load(f)
        
        st.metric("Documentos processados", len(registro))
        
        with st.expander("Ver documentos"):
            for hash_key, info in registro.items():
                st.text(f"üìÑ {info.get('nome', 'N/A')}")
                st.caption(f"   P√°ginas: {info.get('paginas', '?')} | M√©todo: {info.get('metodo', '?')}")

# Fun√ß√£o de inicializa√ß√£o (cache para n√£o recarregar sempre)
@st.cache_resource
def inicializar_sistema(db_path, embedding_model, ollama_model, temperature):
    """Inicializa o sistema RAG"""
    
    # Verificar se banco existe
    if not os.path.exists(db_path):
        st.error(f"‚ùå Banco vetorial n√£o encontrado em: {db_path}")
        st.stop()
    
    # Verificar Ollama
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        if response.status_code != 200:
            raise Exception()
    except:
        st.error("‚ùå Ollama n√£o est√° rodando! Execute: `ollama serve`")
        st.stop()
    
    # Carregar embeddings
    with st.spinner("üß† Carregando modelo de embeddings..."):
        embeddings = HuggingFaceEmbeddings(
            model_name=embedding_model,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )
    
    # Carregar banco
    with st.spinner("üíæ Carregando banco vetorial..."):
        db = Chroma(persist_directory=db_path, embedding_function=embeddings)
    
    # Configurar LLM
    llm = Ollama(
        model=ollama_model,
        temperature=temperature,
        num_ctx=4096,
        num_predict=1024
    )
    
    # Prompt customizado
    template_prompt = """Voc√™ √© um assistente especializado em an√°lise de documentos contratuais e t√©cnicos.

Use APENAS as informa√ß√µes dos documentos fornecidos abaixo para responder √† pergunta.

REGRAS IMPORTANTES:
1. Se a informa√ß√£o N√ÉO estiver nos documentos, diga claramente: "N√£o encontrei essa informa√ß√£o nos documentos fornecidos"
2. Cite especificamente de qual documento e p√°gina voc√™ tirou a informa√ß√£o
3. Se houver informa√ß√µes conflitantes, mencione ambas e suas fontes
4. Seja preciso e objetivo
5. Use linguagem clara e profissional

DOCUMENTOS:
{context}

PERGUNTA: {question}

RESPOSTA DETALHADA:"""

    PROMPT = PromptTemplate(
        template=template_prompt,
        input_variables=["context", "question"]
    )
    
    return db, llm, PROMPT, embeddings

# Inicializar sistema
try:
    db, llm, PROMPT, embeddings = inicializar_sistema(
        db_path, 
        embedding_model, 
        ollama_model, 
        temperature
    )
    
    st.success("‚úÖ Sistema carregado com sucesso!")
    
except Exception as e:
    st.error(f"‚ùå Erro ao inicializar: {e}")
    st.stop()

# Tabs principais
tab1, tab2, tab3 = st.tabs(["üí¨ Consulta", "üîç Busca Literal", "üìä Estat√≠sticas"])

# ============================================================================
# TAB 1: CONSULTA PRINCIPAL
# ============================================================================
with tab1:
    st.header("Fa√ßa sua pergunta sobre os documentos")
    
    # Sugest√µes de perguntas
    with st.expander("üí° Exemplos de perguntas"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **Perguntas diretas:**
            - Qual o prazo do contrato?
            - Quem s√£o as partes contratantes?
            - Qual o valor total do contrato?
            - Quais s√£o as obriga√ß√µes da concession√°ria?
            """)
        
        with col2:
            st.markdown("""
            **Perguntas espec√≠ficas:**
            - H√° previs√£o de multas?
            - Qual o prazo para universaliza√ß√£o?
            - A quem cabe a elabora√ß√£o dos projetos?
            - Quais s√£o as garantias contratuais?
            """)
    
    # Campo de pergunta
    query = st.text_area(
        "Sua pergunta:",
        height=100,
        placeholder="Digite aqui sua pergunta sobre os documentos..."
    )
    
    col1, col2, col3 = st.columns([2, 1, 1])
    
    with col1:
        buscar_btn = st.button("üîç Buscar Resposta", type="primary", use_container_width=True)
    
    with col2:
        mostrar_preview = st.checkbox("Mostrar preview", value=False)
    
    with col3:
        mostrar_fontes = st.checkbox("Mostrar fontes", value=True)
    
    if buscar_btn and query:
        # Configurar retriever
        search_type_str = "mmr" if "MMR" in search_type else "similarity"
        
        if search_type_str == "mmr":
            retriever = db.as_retriever(
                search_type="mmr",
                search_kwargs={
                    "k": num_docs,
                    "fetch_k": num_docs * 3,
                    "lambda_mult": 0.5
                }
            )
        else:
            retriever = db.as_retriever(
                search_type="similarity",
                search_kwargs={"k": num_docs}
            )
        
        # Preview dos documentos encontrados
        if mostrar_preview:
            with st.expander("üìÑ Preview dos documentos encontrados"):
                docs_preview = retriever.invoke(query)
                
                for i, doc in enumerate(docs_preview, 1):
                    st.markdown(f"**[{i}] {doc.metadata.get('source', '?')} (p√°g. {doc.metadata.get('page', '?')})**")
                    st.caption(doc.page_content[:200] + "...")
                    st.divider()
        
        # Criar chain
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            chain_type_kwargs={"prompt": PROMPT}
        )
        
        # Buscar resposta
        with st.spinner("ü§ñ Gerando resposta... (isso pode levar 10-30 segundos)"):
            try:
                result = qa_chain.invoke({"query": query})
                
                # Mostrar resposta
                st.markdown("### üìù Resposta:")
                st.info(result['result'])
                
                # Mostrar fontes
                if mostrar_fontes and result.get('source_documents'):
                    st.markdown("### üìö Fontes utilizadas:")
                    
                    for i, doc in enumerate(result['source_documents'], 1):
                        with st.container():
                            col1, col2, col3 = st.columns([3, 1, 1])
                            
                            with col1:
                                st.markdown(f"**[{i}] {doc.metadata.get('source', 'Desconhecido')}**")
                            
                            with col2:
                                st.caption(f"P√°gina {doc.metadata.get('page', '?')}")
                            
                            with col3:
                                metodo = doc.metadata.get('metodo', '?')
                                badge_color = "üü¢" if metodo == "extra√ß√£o_direta" else "üîµ"
                                st.caption(f"{badge_color} {metodo}")
                            
                            with st.expander("Ver trecho"):
                                st.text(doc.page_content[:500] + "...")
                
                # Salvar no hist√≥rico
                if 'historico' not in st.session_state:
                    st.session_state.historico = []
                
                st.session_state.historico.append({
                    'timestamp': datetime.now().isoformat(),
                    'pergunta': query,
                    'resposta': result['result'],
                    'num_fontes': len(result.get('source_documents', []))
                })
                
            except Exception as e:
                st.error(f"‚ùå Erro ao gerar resposta: {e}")

# ============================================================================
# TAB 2: BUSCA LITERAL
# ============================================================================
with tab2:
    st.header("üîé Busca literal nos documentos")
    st.caption("Busca por termos exatos no texto dos documentos")
    
    termo_busca = st.text_input("Digite o termo para buscar:", placeholder="Ex: universaliza√ß√£o")
    
    if st.button("üîç Buscar Termo", use_container_width=True):
        if termo_busca:
            with st.spinner(f"Buscando '{termo_busca}'..."):
                all_docs = db.get()
                encontrados = []
                
                for doc_id, text, metadata in zip(
                    all_docs['ids'], 
                    all_docs['documents'], 
                    all_docs['metadatas']
                ):
                    if termo_busca.lower() in text.lower():
                        encontrados.append((text, metadata))
                
                if encontrados:
                    st.success(f"‚úÖ Encontrado em {len(encontrados)} trechos")
                    
                    for i, (text, meta) in enumerate(encontrados[:20], 1):  # Mostrar at√© 20
                        source = meta.get('source', '?')
                        page = meta.get('page', '?')
                        
                        with st.expander(f"[{i}] {source} - P√°gina {page}"):
                            # Destacar termo
                            idx = text.lower().find(termo_busca.lower())
                            preview_start = max(0, idx - 100)
                            preview_end = min(len(text), idx + 200)
                            preview = text[preview_start:preview_end]
                            
                            st.text(f"...{preview}...")
                    
                    if len(encontrados) > 20:
                        st.info(f"Mostrando 20 de {len(encontrados)} resultados")
                else:
                    st.warning(f"‚ùå Termo '{termo_busca}' n√£o encontrado")

# ============================================================================
# TAB 3: ESTAT√çSTICAS
# ============================================================================
with tab3:
    st.header("üìä Estat√≠sticas do Sistema")
    
    # Informa√ß√µes do banco
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if os.path.exists("./arquivos_processados.json"):
            with open("./arquivos_processados.json", 'r', encoding='utf-8') as f:
                registro = json.load(f)
            st.metric("üìÑ Documentos", len(registro))
    
    with col2:
        all_docs = db.get()
        st.metric("üì¶ Chunks", len(all_docs['ids']))
    
    with col3:
        if 'historico' in st.session_state:
            st.metric("üí¨ Consultas", len(st.session_state.historico))
    
    # Hist√≥rico de consultas
    if 'historico' in st.session_state and st.session_state.historico:
        st.subheader("üìù Hist√≥rico de Consultas")
        
        for i, item in enumerate(reversed(st.session_state.historico[-10:]), 1):
            with st.expander(f"{i}. {item['pergunta'][:80]}..."):
                st.caption(f"üïê {item['timestamp']}")
                st.markdown(f"**Pergunta:** {item['pergunta']}")
                st.markdown(f"**Resposta:** {item['resposta']}")
                st.caption(f"üìö Fontes consultadas: {item['num_fontes']}")
        
        if st.button("üóëÔ∏è Limpar Hist√≥rico"):
            st.session_state.historico = []
            st.rerun()
    else:
        st.info("Nenhuma consulta realizada ainda")
    
    # Informa√ß√µes t√©cnicas
    with st.expander("üîß Informa√ß√µes T√©cnicas"):
        st.json({
            "Banco Vetorial": db_path,
            "Modelo Embeddings": embedding_model,
            "Modelo LLM": ollama_model,
            "Temperatura": temperature,
            "Tipo de Busca": search_type,
            "Documentos por Busca": num_docs
        })